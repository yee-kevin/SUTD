{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README: LSTM character generator from text sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2w5kKVdm9-om",
    "outputId": "abf46186-8f8d-4bf5-c463-e36864c7ccec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n",
      "(161203, 50)\n",
      "(161203, 1)\n",
      "Train on 128962 samples, validate on 32241 samples\n",
      "Epoch 1/3\n",
      "128962/128962 [==============================] - 194s 2ms/step - loss: 2.2570 - acc: 0.3622 - val_loss: nan - val_acc: 0.3986\n",
      "Epoch 2/3\n",
      "128962/128962 [==============================] - 193s 1ms/step - loss: 1.8900 - acc: 0.4457 - val_loss: nan - val_acc: 0.4403\n",
      "Epoch 3/3\n",
      "128962/128962 [==============================] - 196s 2ms/step - loss: 1.7625 - acc: 0.4802 - val_loss: nan - val_acc: 0.4620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ec26a5b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters, imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Dense, Dropout, Activation, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "# Load training set\n",
    "input_path = colab_data_path + 'shakespeare.txt'\n",
    "with open(input_path) as myfile:\n",
    "    head = [next(myfile) for x in range(4000)] # Load first 4000 lines\n",
    "\n",
    "# Dictionary of character to index e.g. 'a' = 1, 'b' = 5\n",
    "char_index = {}\n",
    "# List of characters in head\n",
    "data = []\n",
    "count = 0\n",
    "for line in head:\n",
    "    chars = list(line)\n",
    "    for i in chars:\n",
    "        # Convert all alphabets to lowercase to prevent duplications\n",
    "        if i.isalpha():\n",
    "            i = i.lower()   \n",
    "        # Do not include line splits '\\n'\n",
    "        if i == '\\n':\n",
    "            continue     \n",
    "        data.append(i)\n",
    "        if i in char_index:\n",
    "            continue     \n",
    "        else:                 \n",
    "            count += 1\n",
    "            char_index[i] = count\n",
    "# List of characters indexed in head\n",
    "data_index = []\n",
    "for i in data:\n",
    "    data_index.append(char_index[i])\n",
    "\n",
    "# Testing prompt\n",
    "seed = \"out of grief and impatience. answer'd neglectingly\"\n",
    "x_test = []\n",
    "for i in seed:\n",
    "    x_test.append(char_index[i])\n",
    "\n",
    "x_train  = []\n",
    "for i in range(len(data_index)-50):\n",
    "    x_train.append(data_index[i:i+50])\n",
    "\n",
    "y_train = []\n",
    "for i in range(50, len(data_index)):\n",
    "    y_train.append(data_index[i])\n",
    "\n",
    "x_test = np.expand_dims(np.asarray(x_test), axis=0)\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.expand_dims(np.asarray(y_train), axis=-1)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_index), EMBEDDING_DIM, trainable=True))\n",
    "model.add(LSTM(\n",
    "\tunits= 128,\n",
    "\tdropout = 0.0, # Adding dropout to input x (same)\n",
    "\trecurrent_dropout = 0.0\n",
    "\t))\n",
    "model.add(Dense(52))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MlMjw1nppSFk",
    "outputId": "1d88497d-d37c-4a17-f477-e4aef6295235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n",
      " \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " t\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " th\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the s\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the sh\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the sha\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the shal\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the shall\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the shall \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the shall t\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " the shall th\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " the shall the\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      " the shall the \n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the w\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wa\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the war\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart \n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart t\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart th\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart the\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " the shall the wart the w\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart the wa\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the war\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart the wart\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart the wart \n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart the wart t\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart th\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      " the shall the wart the wart the\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " the shall the wart the wart the \n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " the shall the wart the wart the w\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " the shall the wart the wart the wa\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart the wart the war\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart \n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart t\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart th\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart the wart the wart the\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart the \n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " the shall the wart the wart the wart the w\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart the wart the wart the wa\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " the shall the wart the wart the wart the war\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " the shall the wart the wart the wart the wart\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " the shall the wart the wart the wart the wart \n",
      "1/1 [==============================] - 0s 23ms/step\n",
      " the shall the wart the wart the wart the wart t\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      " the shall the wart the wart the wart the wart th\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      " the shall the wart the wart the wart the wart the\n"
     ]
    }
   ],
   "source": [
    "# Testing Model\n",
    "predicted_string = \"\"\n",
    "inv_char_index = {v: k for k, v in char_index.items()}\n",
    "test = x_test[0][:].tolist()\n",
    "for i in range(50):      \n",
    "    testing = np.asarray([test])\n",
    "    score = model.predict(testing, batch_size=256, verbose=1)  \n",
    "    predict_index = np.argmax(score,axis=1)   \n",
    "    predict_char = inv_char_index[predict_index[0]] \n",
    "    predicted_string = predicted_string + predict_char   \n",
    "    test.append(predict_index[0])\n",
    "    test = test[1:]   \n",
    "    print(predicted_string)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
